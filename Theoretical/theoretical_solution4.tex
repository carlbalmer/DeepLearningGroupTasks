\documentclass[10pt]{article}
\usepackage{graphicx} 
\usepackage{amsmath}
\usepackage{fancyhdr}
\pagestyle{fancy} 
\fancyhf{} 
\fancyhead[L]{Group Cookie} 
\fancyhead[C]{Assignment 4} 
\fancyhead[R]{} 
\renewcommand{\headrulewidth}{0.4pt} 
\fancyfoot[C]{\thepage} 
\renewcommand{\footrulewidth}{0.4pt}
\begin{document}
\section{Theoretical Exercise}
\subsection{Task 1}
The number of parameters can be computed with the following formula : 
$n^2+kn+nm$, where n is the dimension of the hidden layer, k is the dimension of the output layer and m is the dimension of the input layer. Using this formula, the number of parameters is 20224. 

Since the number of parameters is shared by all the steps of the RNN, the number of parameters stays the same.

\subsection{Task 2}
\subsubsection{Question 1}
The problem is with vanishing gradients; to model long term dependencies the back propagation step in the training has to go through many time steps to adjust the weights at the initial time steps. Since, according to the chain rule, the back propagation involves multiplying many gradients which can lead to a gradient, which can tend to zero. A near zero gradient, however is very suboptimal for the gradient step. 
\subsubsection{Question 2}


\subsection{LSTM Task 1 }
\subsubsection{Question 1}
Gates are a way to optionally let information through; by allowing to let this happen LSTM combat the problem that RNN encountered when modelling long term dependencies. 
\subsubsection{Question 2}
To update the old cell state to the new state following transition is computed : 

$C_t = f_t * C_{t-1}+i_t * \widetilde{C_t}$. If the forgot gate is zero, only the new candidate values are considered by the weight $i_t$

\subsubsection{Question 3}
LSTM solves the vanishing gradient problem through the gating mechanism; it allows the information to directly pass through (mathematically this means applying an identity function over the inputs and because the gradient of the identity function is 1, the problem of vanishing is reduced by a huge margin)

Another way to look at it is : \\
n the recurrency of the LSTM the activation function is the identity function with a derivative of 1.0. So, the backpropagated gradient neither vanishes or explodes when passing through, but remains constant.
The effective weight of the recurrency is equal to the forget gate activation. So, if the forget gate is on (activation close to 1.0), then the gradient does not vanish. Since the forget gate activation is never greater than 1.0, the gradient can't explode either.
So that's why LSTM is so good at learning long range dependencies. 

\subsubsection{Question 4}
A standard LSTM has 3 gates; namely input,forget and output gate which introduce a lot of parameters and make training quite a hassle.
One way to reduce the parameters in the LSTM is by simplified gating which was introduced by GRUs. Instead of having two gates, they consider two gates (namely update and reset gate) (needs more)


\end{document}
